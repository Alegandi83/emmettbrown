{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "887db04b-8b2f-4508-8d38-c84b4a393734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE dad_open_data.time.temporal_dimension (\n",
    "  year INT,\n",
    "  month INT,\n",
    "  day INT,\n",
    "  quarter INT,\n",
    "  week_of_year INT,\n",
    "  day_of_week INT,\n",
    "  day_name STRING,\n",
    "  month_name STRING,\n",
    "  iso_date STRING,\n",
    "  is_leap_year BOOLEAN,\n",
    "  is_weekend BOOLEAN\n",
    ");\n",
    "\n",
    "INSERT INTO dad_open_data.time.temporal_dimension\n",
    "SELECT\n",
    "  y AS year,\n",
    "  m AS month,\n",
    "  d AS day,\n",
    "  CEIL(m / 3.0) AS quarter,\n",
    "  weekofyear(TO_DATE(LPAD(CASE WHEN y < 1 THEN 1 ELSE y END, 4, '0') || '-' || LPAD(m,2,'0') || '-' || LPAD(d,2,'0'))) AS week_of_year,\n",
    "  dayofweek(TO_DATE(LPAD(CASE WHEN y < 1 THEN 1 ELSE y END, 4, '0') || '-' || LPAD(m,2,'0') || '-' || LPAD(d,2,'0'))) AS day_of_week,\n",
    "  date_format(TO_DATE(LPAD(CASE WHEN y < 1 THEN 1 ELSE y END, 4, '0') || '-' || LPAD(m,2,'0') || '-' || LPAD(d,2,'0')), 'EEEE') AS day_name,\n",
    "  date_format(TO_DATE(LPAD(CASE WHEN y < 1 THEN 1 ELSE y END, 4, '0') || '-' || LPAD(m,2,'0') || '-' || LPAD(d,2,'0')), 'MMMM') AS month_name,\n",
    "  CASE \n",
    "    WHEN y < 1 THEN LPAD(ABS(y)+1, 4, '0') || '-' || LPAD(m,2,'0') || '-' || LPAD(d,2,'0') || ' BC'\n",
    "    ELSE LPAD(y, 4, '0') || '-' || LPAD(m,2,'0') || '-' || LPAD(d,2,'0')\n",
    "  END AS iso_date,\n",
    "  CASE \n",
    "    WHEN (y > 0 AND ((y % 4 = 0 AND y % 100 != 0) OR (y % 400 = 0))) THEN TRUE\n",
    "    ELSE FALSE\n",
    "  END AS is_leap_year,\n",
    "  CASE \n",
    "    WHEN dayofweek(TO_DATE(LPAD(CASE WHEN y < 1 THEN 1 ELSE y END, 4, '0') || '-' || LPAD(m,2,'0') || '-' || LPAD(d,2,'0'))) IN (1,7) THEN TRUE\n",
    "    ELSE FALSE\n",
    "  END AS is_weekend\n",
    "FROM (\n",
    "  SELECT\n",
    "    y,\n",
    "    m,\n",
    "    d\n",
    "  FROM\n",
    "    (\n",
    "      SELECT explode(sequence(-3999, 3000)) AS y\n",
    "    ) years\n",
    "    CROSS JOIN (\n",
    "      SELECT explode(sequence(1, 12)) AS m\n",
    "    ) months\n",
    "    CROSS JOIN (\n",
    "      SELECT explode(sequence(1, 31)) AS d\n",
    "    ) days\n",
    "  WHERE\n",
    "    -- Filter out invalid dates\n",
    "    (m IN (1,3,5,7,8,10,12) AND d <= 31)\n",
    "    OR (m IN (4,6,9,11) AND d <= 30)\n",
    "    OR (m = 2 AND d <= \n",
    "      CASE \n",
    "        WHEN (y > 0 AND ((y % 4 = 0 AND y % 100 != 0) OR (y % 400 = 0))) THEN 29\n",
    "        ELSE 28\n",
    "      END\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ccf2a6-9883-47ef-8f35-28dbf3615ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, date_format, dayofweek, dayofyear, weekofyear, quarter, month, dayofmonth, year, floor, pmod\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Parametri tabella\n",
    "catalogo = \"dad_open_data\"\n",
    "schema = \"time\"\n",
    "tabella = \"calendar\"\n",
    "full_table_name = f\"{catalogo}.{schema}.{tabella}\"\n",
    "\n",
    "# Crea catalogo e schema se non esistono\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalogo}\")\n",
    "spark.sql(f\"USE CATALOG {catalogo}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n",
    "\n",
    "print(f\"Generazione date dal 3999 a.C. al 2100 d.C...\")\n",
    "\n",
    "# Date supportate da datetime (1-01-01 al 2100-12-31)\n",
    "data_inizio = datetime(1, 1, 1)\n",
    "data_fine = datetime(2100, 12, 31)\n",
    "date_supportate = []\n",
    "data_corrente = data_inizio\n",
    "while data_corrente <= data_fine:\n",
    "    date_supportate.append(data_corrente)\n",
    "    data_corrente += timedelta(days=1)\n",
    "\n",
    "df_pandas = pd.DataFrame({'data': date_supportate})\n",
    "df_date_supportate = spark.createDataFrame(df_pandas)\n",
    "\n",
    "df_calendario_supportate = df_date_supportate.select(\n",
    "    year(col('data')).alias('anno'),\n",
    "    month(col('data')).alias('mese'),\n",
    "    dayofmonth(col('data')).alias('giorno'),\n",
    "    quarter(col('data')).alias('trimestre'),\n",
    "    (floor((month(col('data')) - 1) / 3) * 3 + 1).alias('mese_inizio_trimestre'),\n",
    "    (floor(pmod(month(col('data')), 12) / 3) + 1).alias('stagione'),\n",
    "    dayofweek(col('data')).alias('giorno_settimana_num'),\n",
    "    date_format(col('data'), 'EEEE').alias('nome_giorno'),\n",
    "    dayofyear(col('data')).alias('giorno_anno'),\n",
    "    weekofyear(col('data')).alias('settimana_anno'),\n",
    "    col('data')\n",
    ")\n",
    "\n",
    "# Genera date BCE (3999 a.C. al 0 d.C.)\n",
    "date_bce = []\n",
    "for anno in range(-3999, 1):  # Da -3999 a 0\n",
    "    for mese in range(1, 13):\n",
    "        if mese in [1, 3, 5, 7, 8, 10, 12]:\n",
    "            giorni_mese = 31\n",
    "        elif mese in [4, 6, 9, 11]:\n",
    "            giorni_mese = 30\n",
    "        else:\n",
    "            anno_abs = abs(anno)\n",
    "            if (anno_abs % 4 == 0 and anno_abs % 100 != 0) or (anno_abs % 400 == 0):\n",
    "                giorni_mese = 29\n",
    "            else:\n",
    "                giorni_mese = 28\n",
    "        for giorno in range(1, giorni_mese + 1):\n",
    "            date_bce.append({\n",
    "                'anno': anno,\n",
    "                'mese': mese,\n",
    "                'giorno': giorno,\n",
    "                'trimestre': (mese - 1) // 3 + 1,\n",
    "                'mese_inizio_trimestre': ((mese - 1) // 3) * 3 + 1,\n",
    "                'stagione': (mese % 12) // 3 + 1,\n",
    "                'giorno_settimana_num': None,\n",
    "                'nome_giorno': None,\n",
    "                'giorno_anno': None,\n",
    "                'settimana_anno': None,\n",
    "                'data': None\n",
    "            })\n",
    "\n",
    "schema_bce = StructType([\n",
    "    StructField('anno', IntegerType(), True),\n",
    "    StructField('mese', IntegerType(), True),\n",
    "    StructField('giorno', IntegerType(), True),\n",
    "    StructField('trimestre', IntegerType(), True),\n",
    "    StructField('mese_inizio_trimestre', IntegerType(), True),\n",
    "    StructField('stagione', IntegerType(), True),\n",
    "    StructField('giorno_settimana_num', IntegerType(), True),\n",
    "    StructField('nome_giorno', StringType(), True),\n",
    "    StructField('giorno_anno', IntegerType(), True),\n",
    "    StructField('settimana_anno', IntegerType(), True),\n",
    "    StructField('data', DateType(), True)\n",
    "])\n",
    "df_bce = spark.createDataFrame(date_bce, schema=schema_bce)\n",
    "\n",
    "df_calendario_completo = df_bce.unionByName(df_calendario_supportate)\n",
    "\n",
    "df_calendario_finale = df_calendario_completo.withColumn(\n",
    "    'nome_stagione',\n",
    "    col('stagione').cast('string')\n",
    ").replace(\n",
    "    ['1', '2', '3', '4'],\n",
    "    ['Primavera', 'Estate', 'Autunno', 'Inverno'],\n",
    "    subset=['nome_stagione']\n",
    ")\n",
    "\n",
    "df_calendario_finale = df_calendario_finale.select(\n",
    "    'data',\n",
    "    'anno',\n",
    "    'mese',\n",
    "    'giorno',\n",
    "    'trimestre',\n",
    "    'mese_inizio_trimestre',\n",
    "    'stagione',\n",
    "    'nome_stagione',\n",
    "    'giorno_settimana_num',\n",
    "    'nome_giorno',\n",
    "    'giorno_anno',\n",
    "    'settimana_anno'\n",
    ").orderBy('anno', 'mese', 'giorno')\n",
    "\n",
    "total_records = df_calendario_finale.count()\n",
    "print(f\"Record totali generati: {total_records:,}\")\n",
    "\n",
    "df_calendario_finale.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(full_table_name)\n",
    "\n",
    "print(f\"âœ“ Tabella {full_table_name} creata con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ba48138-7f23-4928-b9c5-dbc1f92a3ecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5516379330374743,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "00_create",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
